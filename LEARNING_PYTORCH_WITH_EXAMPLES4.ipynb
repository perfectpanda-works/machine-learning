{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LEARNING PYTORCH WITH EXAMPLES4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhLpP0KSdJMeTNyIilF+7g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perfectpanda-works/machine-learning/blob/master/LEARNING_PYTORCH_WITH_EXAMPLES4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6s1iYC1yhF3",
        "colab_type": "text"
      },
      "source": [
        "#nnモジュール\n",
        "計算グラフとautogradは、複雑な演算子を定義して自動的に導関数をとるための非常に強力なパラダイムです。\n",
        "ただし、大規模なニューラルネットワークの場合、直接autogradを利用すると、勾配の算出などの直接的な仕組みを考えなければならないので、もう少し抽象化された概念として扱いたいです。\n",
        "\n",
        "\n",
        "ニューラルネットワークを扱うときには、入力層、中間層、出力層などのように、層としてネットワークを考え、それぞれのノードには「重み」や「バイアス」という訓練可能なパラメータを持つモデルとして考えたいです。\n",
        "\n",
        "TensorFlowでは、「Keras、TensorFlow-Slim、TFLearn」などのパッケージが計算グラフなどの直接的な実装を隠蔽して、より抽象的にニューラルネットワークを扱えるようにしています。\n",
        "\n",
        "PyTorchではnnパッケージがこの抽象化を行います。nnパッケージは、ニューラルネットワークの層の概念と同等の一連の「モジュール」を提供してくれます。\n",
        "モジュールは、入力テンソルを受信して​​出力テンソルを計算します。学習可能なパラメータを含むテンソルなどの内部状態も保持する場合があります。\n",
        "\n",
        "nnパッケージは、ニューラルネットワークのトレーニング時に一般的に使用される有用な損失関数も定義します。（例えば平均二乗誤差）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhoVS2dSELvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "\n",
        "# N  　：バッチサイズ\n",
        "# D_in ：入力次元数\n",
        "# H　　：隠れ層の次元数\n",
        "# D_out：出力次元数\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# ランダムな入力データと出力データの作成\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG6mXfbcFtef",
        "colab_type": "text"
      },
      "source": [
        "nnモジュールを利用して、ネットワークの流れをモデルとして定義します。\n",
        "\n",
        "nn.Linearは線形変換を行います。入力、出力を引数に取ります。\n",
        "ReLUは、ReLUでの変換を行います。２層のニューラルネットワークでは、次のようなモデルになります。もちろん、訓練パラメータも保持しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBaXdnTDEsy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ニューラルネットワークの定義\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")\n",
        "\n",
        "#平均二乗誤差\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGdS6Ew-yYzJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "71c8f93e-6202-49e2-b251-a53fb857f3a1"
      },
      "source": [
        "learning_rate = 1e-4\n",
        "for t in range(500):\n",
        "    # ①順伝播: 入力を与えて順伝播させます。\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # ②損失を計算します。\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # ③逆伝播の前に勾配を初期化します。\n",
        "    model.zero_grad()\n",
        "\n",
        "    # 逆伝播:\n",
        "    loss.backward()\n",
        "\n",
        "    # ④重みの更新. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 1.7279762029647827\n",
            "199 0.027289215475320816\n",
            "299 0.0008932463242672384\n",
            "399 3.5993754863739014e-05\n",
            "499 1.688112320152868e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CzhcJ-QNzip",
        "colab_type": "text"
      },
      "source": [
        "①順伝播をシンプルに表記できています。\n",
        "\n",
        "②上で定義したMESLossを変更するだけで簡単にいろいろな誤差を試すことができるようになりました。\n",
        "\n",
        "③逆伝播の勾配初期化は必要です。また、最初に定義したモデルが「requires_grad = True」としているため、最終的な計算結果であるlossをbackwardすることで微分値である勾配を求めることができます。\n",
        "\n",
        "④nnモジュールでニューラルネットワークを定義した場合、model.parameters()のようにパラメータを取得することができますので、for文ですべてのパラメータにアクセスしてそれぞれに勾配降下法で重みを更新していきます。"
      ]
    }
  ]
}